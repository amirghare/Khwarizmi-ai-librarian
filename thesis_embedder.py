import pandas as pd
import numpy as np
import faiss
import pickle
import time
from pathlib import Path
from langchain_openai import OpenAIEmbeddings
import config

# Paths
THESES_EXCEL = "output/theses/theses_normalized.xlsx"
THESES_INDEX = "output/theses/faiss_index.bin"
EMBEDDING_MODEL = "text-embedding-3-small"


class ThesisEmbedder:
    def __init__(self, api_key):
        print("ğŸ”§ Initializing embedder...")
        self.embedding_client = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=api_key
        )
        self.index = None
        self.metadata_map = {}
        print("âœ… Embedder ready.")

    def create_description(self, row):
        parts = []

        # Title (x3 weight)
        title = self._clean(row.get('Ø¹Ù†ÙˆØ§Ù† Ù¾Ø§ÛŒØ§Ù†â€ŒÙ†Ø§Ù…Ù‡') or row.get('Ø¹Ù†ÙˆØ§Ù†'))
        if title:
            parts.extend([title, title, title])

        # Advisor (x3)
        advisor = self._clean(row.get('Ø§Ø³ØªØ§Ø¯ Ø±Ø§Ù‡Ù†Ù…Ø§'))
        if advisor:
            advisor_clean = advisor.replace('Ø¯Ú©ØªØ±', '').replace('Ø¯ÙƒØªØ±', '').strip()
            parts.extend([
                f"Ø§Ø³ØªØ§Ø¯ Ø±Ø§Ù‡Ù†Ù…Ø§ {advisor_clean}",
                f"Ø±Ø§Ù‡Ù†Ù…Ø§ {advisor_clean}",
                advisor_clean
            ])

        # Coâ€‘advisor (x2)
        co_advisor = self._clean(row.get('Ø§Ø³ØªØ§Ø¯ Ù…Ø´Ø§ÙˆØ±'))
        if co_advisor:
            co_advisor_clean = co_advisor.replace('Ø¯Ú©ØªØ±', '').replace('Ø¯ÙƒØªØ±', '').strip()
            parts.extend([
                f"Ø§Ø³ØªØ§Ø¯ Ù…Ø´Ø§ÙˆØ± {co_advisor_clean}",
                co_advisor_clean
            ])

        # Author (x2)
        author = self._clean(row.get('Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡') or row.get('Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±'))
        if author:
            parts.extend([f"Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ {author}", author])

        # Major (x2)
        field = self._clean(row.get('Ø±Ø´ØªÙ‡') or row.get('Ø±Ø´ØªÙ‡ ØªØ­ØµÛŒÙ„ÛŒ'))
        if field:
            parts.extend([f"Ø±Ø´ØªÙ‡ {field}", field])

        # Keywords (x2)
        keywords = self._clean(row.get('Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§') or row.get('ØªÙˆØµÛŒÙÚ¯Ø±'))
        if keywords:
            kw_list = keywords.replace('ØŒ', ',').split(',')
            for kw in kw_list[:5]:  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡
                kw = kw.strip()
                if kw:
                    parts.extend([kw, kw])

        # Degree (x1)
        degree = self._clean(row.get('Ù…Ù‚Ø·Ø¹'))
        if degree:
            parts.append(f"Ù…Ù‚Ø·Ø¹ {degree}")

        # Faculty (x1)
        faculty = self._clean(row.get('Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡'))
        if faculty:
            parts.append(f"Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ {faculty}")

        # Year (x1)
        year = self._clean(row.get('Ø³Ø§Ù„') or row.get('Ø³Ø§Ù„ Ø¯ÙØ§Ø¹'))
        if year:
            parts.append(f"Ø³Ø§Ù„ {year}")

        description = " ".join(parts)
        description = ' '.join(description.split())

        return description if description else "No description"

    def _clean(self, value):
        if pd.isna(value) or value is None:
            return None
        value_str = str(value).strip()
        if value_str.lower() in ['nan', 'none', '', 'null']:
            return None
        return value_str

    def prepare_data(self, excel_path):
        print(f"ğŸ“– Loading Excel file: {excel_path}")

        try:
            df = pd.read_excel(excel_path, engine='openpyxl')
            print(f"âœ… Loaded {len(df)} theses")
        except Exception as e:
            print(f"âŒ Error loading file: {e}")
            return None

        df.columns = [col.strip() for col in df.columns]

        print("ğŸ”„ Building descriptions...")
        df['description'] = df.apply(self.create_description, axis=1)

        records = []
        for idx, row in df.iterrows():
            if 'Ø±Ø¯ÙŠÙ' in row or 'Ø±Ø¯ÛŒÙ' in row:
                record_id = int(row.get('Ø±Ø¯ÙŠÙ') or row.get('Ø±Ø¯ÛŒÙ') or idx)
            else:
                record_id = idx

            record = {
                'id': record_id,
                'text': row['description'],
                'metadata': {
                    'Ø±Ø¯ÙŠÙ': record_id,
                    'Ø¹Ù†ÙˆØ§Ù† Ù¾Ø§ÛŒØ§Ù†â€ŒÙ†Ø§Ù…Ù‡': self._clean(row.get('Ø¹Ù†ÙˆØ§Ù† Ù¾Ø§ÛŒØ§Ù†â€ŒÙ†Ø§Ù…Ù‡') or row.get('Ø¹Ù†ÙˆØ§Ù†')) or '',
                    'Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡': self._clean(row.get('Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡')) or '',
                    'Ø§Ø³ØªØ§Ø¯ Ø±Ø§Ù‡Ù†Ù…Ø§': self._clean(row.get('Ø§Ø³ØªØ§Ø¯ Ø±Ø§Ù‡Ù†Ù…Ø§')) or '',
                    'Ø§Ø³ØªØ§Ø¯ Ù…Ø´Ø§ÙˆØ±': self._clean(row.get('Ø§Ø³ØªØ§Ø¯ Ù…Ø´Ø§ÙˆØ±')) or '',
                    'Ø±Ø´ØªÙ‡': self._clean(row.get('Ø±Ø´ØªÙ‡') or row.get('Ø±Ø´ØªÙ‡ ØªØ­ØµÛŒÙ„ÛŒ')) or '',
                    'Ù…Ù‚Ø·Ø¹': self._clean(row.get('Ù…Ù‚Ø·Ø¹')) or '',
                    'Ø³Ø§Ù„': self._clean(row.get('Ø³Ø§Ù„') or row.get('Ø³Ø§Ù„ Ø¯ÙØ§Ø¹')) or '',
                }
            }
            records.append(record)

        print(f"âœ… Prepared {len(records)} records")
        return records

    def create_embeddings(self, records, batch_size=200):
        print("ğŸ”„ Generating embeddings...")
        print("â³ This may take a few minutes...")

        texts = [r['text'] for r in records]
        total_texts = len(texts)
        all_vectors = []

        try:
            for i in range(0, total_texts, batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (total_texts + batch_size - 1) // batch_size

                print(f"ğŸŒ Batch {batch_num}/{total_batches} ({len(batch_texts)} texts)...")

                batch_vectors = self.embedding_client.embed_documents(batch_texts)
                all_vectors.extend(batch_vectors)

                processed = min(i + batch_size, total_texts)
                progress = (processed / total_texts) * 100
                print(f"   âœ… {processed}/{total_texts} ({progress:.1f}%)")

                if i + batch_size < total_texts:
                    time.sleep(0.2)

            vectors_array = np.array(all_vectors, dtype='float32')
            print(f"\nâœ… Generated {len(all_vectors)} embeddings")
            print(f"ğŸ“Š Vector dimensions: {vectors_array.shape}")
            return vectors_array

        except Exception as e:
            print(f"\nâŒ Error: {e}")
            return None

    def build_faiss_index(self, vectors, records):
        print("ğŸ”„ Building FAISS index...")

        dimension = vectors.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index = faiss.IndexIDMap(index)

        ids = np.array([r['id'] for r in records], dtype='int64')
        index.add_with_ids(vectors, ids)

        self.metadata_map = {r['id']: r['metadata'] for r in records}

        print("âœ… FAISS index built")
        print(f"ğŸ“Š Total vectors: {index.ntotal}")

        self.index = index
        return index

    def save_index(self, index_path):
        print(f"ğŸ’¾ Saving index: {index_path}")

        Path(index_path).parent.mkdir(parents=True, exist_ok=True)

        faiss.write_index(self.index, index_path)

        metadata_path = index_path.replace('.bin', '_metadata.pkl')
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.metadata_map, f)

        print(f"âœ… Saved:")
        print(f"   ğŸ“ {index_path}")
        print(f"   ğŸ“ {metadata_path}")


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Thesis Embedding - FIXED VERSION")
    print("=" * 60)

    try:
        embedder = ThesisEmbedder(api_key=config.OPENAI_API_KEY)
    except Exception as e:
        print(f"âŒ Error: {e}")
        exit(1)

    records = embedder.prepare_data(THESES_EXCEL)
    if records is None:
        print("âŒ Data preparation failed")
        exit(1)

    vectors = embedder.create_embeddings(records)
    if vectors is None:
        print("âŒ Embedding generation failed")
        exit(1)

    embedder.build_faiss_index(vectors, records)

    embedder.save_index(THESES_INDEX)

    print("\n" + "=" * 60)
    print("âœ… Done!")
    print("=" * 60)
